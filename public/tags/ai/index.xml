<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Daker Pinheiro</title>
    <link>http://dakerfp.github.io/tags/ai/</link>
    <description>Recent content in Ai on Daker Pinheiro</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Daker Fernandes Pinheiro</copyright>
    <lastBuildDate>Sun, 05 Jun 2011 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://dakerfp.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>CpG Islands (3) - Model Evaluation</title>
      <link>http://dakerfp.github.io/post/cpg-islands-model-evaluation-3/</link>
      <pubDate>Sun, 05 Jun 2011 00:00:00 +0000</pubDate>
      
      <guid>http://dakerfp.github.io/post/cpg-islands-model-evaluation-3/</guid>
      <description>&lt;p&gt;Following the &lt;a href=&#34;http://codecereal.blogspot.com/2011/06/cpg-islands-2.html&#34;&gt;post&lt;/a&gt;
we&amp;rsquo;ve built a Hidden Markov Model (HMM) for the &lt;a href=&#34;http://codecereal.blogspot.com/2011/05/cpg-islands-1.html&#34;&gt;CpG islands problem&lt;/a&gt;, using the training set.
Now we should evaluate if our model is adequate to predict things about CpG islands.
For evaluate we may use a tagged
sequence and see if the HMM we built can predict the islands and what is
it&amp;rsquo;s accuracy.&lt;/p&gt;

&lt;p&gt;Using the viterbi path and a tagged sequence (out ouf the training set),
enable us to compare if the estimative given by the model is coherent
with the real data. Using the HMM and the training and test sets of the
last post, we can compare the results using a &lt;a href=&#34;http://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;confusion
matrix&lt;/a&gt;. For example, at
the following snippet of the viterbi path paired with the tagged
sequence:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ ... 0,0,0,0,0,1,1,1,1,1, ... ]
[ ... a t t g t c C G A C  ... ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We may calculate the following confusion matrix:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+--------------------------------------+ 
|            |  Island    | Non Island |
+------------+------------+------------+
| Island     |     4      |      0     |
+------------+------------+------------+
| Non Island |     1      |      5     |
+------------+------------+------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the bigger are the main diagonal values, the more accurate our
model is. Making this matrix with the experiment made (using
&lt;a href=&#34;http://codecereal.blogspot.com/2011/06/cpg-islands-2.html&#34;&gt;HMM with maximum likelihood&lt;/a&gt;)
I got the following matrix:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+--------------------------------------+ 
|            |  Island    | Non Island |
+------------+------------+------------+
| Island     |   328815   |    61411   |
+------------+------------+------------+
| Non Island |  1381515   |   1889819  |
+------------+------------+------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We may see that a good amount of islands was correctly predicted (84,26%
of the islands), but 57,76% of the non islands regions was classified as
islands, which is a bad indicator. A experiment with the HMM after
running the Baum Welch was also evaluated. The results was better from
the obtained by the maximum likelihood when predicting islands (96,56%
of accuracy). But also obtained a higher miss rate (70,70%). Here is the
confusion matrix for this HMM:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+--------------------------------------+ 
|            |  Island    | Non Island |
+------------+------------+------------+
| Island     |   376839   |    13387   |
+------------+------------+------------+
| Non Island |  2313138   |   958196   |
+------------+------------+------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://dakerfp.github.io/img/cpgs_labels.png&#34; alt=&#34;Matching of Real Sequence, Viterbi and Posterior results&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The result obtained was not very accurate. It may happened because we
had few data to train and to evaluate our model. We could also build a
HMM with other indicators that identify the CpG islands for model it
better. Remember we simplified the CpG problem to the frequencies of
occurring Cs and Gs, but a better model could also use the CG pairs.
Using a more complicate HMM we could have more than 2 states and then
associate a set of states to the CpG and non CpG islands sites. This
would allow to use better the posterior result to classify the sequence.
But I keep this as future work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CpG Islands (2) - Building a Hidden Markov Model</title>
      <link>http://dakerfp.github.io/post/cpg-islands-model-evaluation-2/</link>
      <pubDate>Thu, 02 Jun 2011 00:00:00 +0000</pubDate>
      
      <guid>http://dakerfp.github.io/post/cpg-islands-model-evaluation-2/</guid>
      <description>&lt;p&gt;By the definition of the CpG islands in the
&lt;a href=&#34;http://codecereal.blogspot.com/2011/05/cpg-islands-1.html&#34;&gt;previous post&lt;/a&gt;
and the &lt;a href=&#34;http://codecereal.blogspot.com/2011/05/hidden-markov-models.html&#34;&gt;Hidden Markov Models&lt;/a&gt;
(HMMs) short introduction, we now can model a HMM for finding CpG
islands. We can create a model very similar to the &amp;ldquo;Fair Bet Casino&amp;rdquo;
problem.&lt;/p&gt;

&lt;p&gt;When we are in a nucleotide of given DNA sequence there are two
possibilities, that nucleotide belongs to CpG island (lets denote state
S1) or do not (S0). If you analyse a sibling nucleotide it can stay in
the same state or to change with complementary probabilities. We can
view it as this Markov chain:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dakerfp.github.io/img/cpg.png&#34; alt=&#34;CpG states Markov chain&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the emissions of the states S0 and S1, we may have the associated
probabilities of the emissions of ACGT symbols. However we can&amp;rsquo;t know
for now how much are these probabilities. However if we have a dataset
of sequences with the attached information of where the CpG islands
occurs, we can estimate those parameters. I used these DNA
&lt;a href=&#34;http://www.cin.ufpe.br/%7Eigcf&#34;&gt;sequences&lt;/a&gt; tagged by biologists (used
&lt;a href=&#34;http://biopython.org/wiki/Main_Page&#34;&gt;biopython&lt;/a&gt; library to parse the
&lt;a href=&#34;http://en.wikipedia.org/wiki/FASTA_format&#34;&gt;.fasta&lt;/a&gt; files) as the
training and test sets to build the model and evaluate the technique.
This dataset consists of nucleotides in upper and lower cases. When we
have a given nucleotide in upper case, it denotes that it is a CpG site
and the lower case nucleotides means they are not. I used a statistical
technique called &lt;a href=&#34;http://en.wikipedia.org/wiki/Maximum_likelihood&#34;&gt;maximum likelihood&lt;/a&gt;
to build the HMM.&lt;/p&gt;

&lt;p&gt;Using the maximum likelihood to calculate the HMM emissions
probabilities, I count each char frequency in the dataset and divide by
the total amount of nucleotides in the same state:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dakerfp.github.io/img/cpgemissions.png&#34; alt=&#34;Emissions probability diagram&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P(A|S0) = a / (a+c+g+t) P(C|S0) = c / (a+c+g+t)
P(G|S0) = g / (a+c+g+t) P(T|S0) = t / (a+c+g+t)
P(A|S1) = A / (A+C+G+T) P(C|S1) = C / (A+C+G+T)
P(G|S1) = G / (A+C+G+T) P(T|S1) = T / (A+C+G+T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And to calculate the transition probabilities of each state, count the
number of transitions between a CpG to a non CpG site ``(out_of) and
the reversal transitions (into). Then divide each of them by the state
frequency which they&amp;rsquo;re coming from. Note that the permanence
probability of each state is the complement of the probability of
transiting between states since there is no other state to go:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P(S0|S1) = out_of / (A+C+G+T)
P(S1|S0) = into / (a+c+g+t)
P(S0|S0) = 1 - P(S1|S0)
P(S1|S1) = 1 - P(S0|S1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our model is almost done, it just lacks of the initial transitions which
can be supposed to be the frequency of each state over all nucleotides:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P(S0) = a + c + g + t / (a + c + g + t + A + C + G + T)
P(S1) = A + C + G + T / (a + c + g + t + A + C + G + T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I used the &lt;a href=&#34;http://ghmm.org/&#34;&gt;ghmm&lt;/a&gt; to create my HMM with the given
dataset and it generated this model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DiscreteEmissionHMM(N=2, M=4)
	state 0 (initial=0.97)
       Emissions: 0.30, 0.21, 0.20, 0.29
       Transitions: -&amp;gt;0 (0.00), -&amp;gt;1 (1.00)
   state 1 (initial=0.03)
       Emissions: 0.22, 0.29, 0.29, 0.20
       Transitions: -&amp;gt;0 (1.00), -&amp;gt;1 (0.00)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s important to notice that when we print the HMM it says that has the
probability of 1 to stay in the same state. This occurs due to float
formatted print. The reality is that in that value is very close to 1
but still has a chance to hop between states. It&amp;rsquo;s also interesting to
see the probability of emission of C and G when we contrast the states.
In S1 the probability of getting a C or a G is significantly higher than
in S0.&lt;/p&gt;

&lt;p&gt;Another manner of creating the HMM, is by initializing a model with the
same structure we modelled the problem and then apply some unsupervised
learning algorithms, such as the
&lt;a href=&#34;http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm&#34;&gt;Baum Welch&lt;/a&gt;
algorithm. The Baum Welch uses randomized initial weights in the HMM and
once you feed untagged sequences, it tries to infer a model. However,
usually is far better initialize your HMM with biased data about the
problem (for example using the model we created) and let the Baum Welch
calibrate the probabilities. With ghmm do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sigma = ghmm.IntegerRange(0,4) # our emission alphabet
emission_seq = ghmm.EmissionSequence(sigma, [0, 4, 5, ... ]) # create a emission sequence hmm.
baumWelch(sigma, emission_seq) # use a template hmm to calibrate the probablities
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the next post I will show how is the evaluation process of the HMM
in the CpG island.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hidden Markov Models</title>
      <link>http://dakerfp.github.io/post/hidden-markov-models/</link>
      <pubDate>Mon, 30 May 2011 00:00:00 +0000</pubDate>
      
      <guid>http://dakerfp.github.io/post/hidden-markov-models/</guid>
      <description>&lt;p&gt;Nowadays, many applications use
&lt;a href=&#34;http://en.wikipedia.org/wiki/Hidden_Markov_model&#34;&gt;Hidden Markov Models&lt;/a&gt;
(HMMs) to solve crucial issues such as bioinformatics, speech recognition,
musical analysis, digital signal processing, data mining, financial
applications, time series analysis and many others. HMMs are
probabilistic models which are very useful to model sequence behaviours
or discrete time series events. Formally it models
&lt;a href=&#34;http://en.wikipedia.org/wiki/Markov_process&#34;&gt;Markov processes&lt;/a&gt;
with hidden states, like an extension for
&lt;a href=&#34;http://en.wikipedia.org/wiki/Markov_chain&#34;&gt;Markov Chains&lt;/a&gt;.
For computer scientists, is a state machine with probabilistic transitions
where each state can emit a value with a given probability.&lt;/p&gt;

&lt;p&gt;For better understanding HMMs, I will illustrate how it works with &amp;ldquo;The
Fair Bet Casino&amp;rdquo; problem. Imagine you are in a casino where you can bet
on coins tosses, tossed by a dealer. A coin toss can have two outcomes:
head (H) or tail (T). Now suppose that the coin dealer has two coins, a
fair (F) which outputs both H and T with &lt;code&gt;1/2&lt;/code&gt; probabilities and a biased
coin (B) which outputs H with probability &lt;code&gt;3/4&lt;/code&gt; and T with &lt;code&gt;1/4&lt;/code&gt;. Using
probability language we say:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(H~i+1~|F~i~) = 1/2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(T~i+1~|F~i~) = 1/2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(H~i+1~|B~i~) = 3/4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(T~i+1~|B~i~) = 1/4&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now imagine that the dealer changes the coin in a way you can&amp;rsquo;t see, but
you know that he does it with a &lt;code&gt;1/10&lt;/code&gt; probability. So thinking the coin
tosses as a sequence of events we can say:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(F~i+1~|F~i~) = 9/10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(B~i+1~|F~i~) = 1/10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(B~i+1~|B~i~) = 9/10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(F~i+1~|B~i~) = 1/10&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can model it using a graph to illustrate the process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dakerfp.github.io/img/fairbet.png&#34; alt=&#34;HMM for &amp;quot;The Fair Bet Casino&amp;quot; problem&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s a HMM! It isn&amp;rsquo;t any rocket science. Is just important to add a
few remarks. We call the set of all possible emissions of the Markov
process as the alphabet Σ ({H, T} in our problem). For many of
computational method involving HMMs you will also need a initial state
distribution π. For our problem we may assume that the we have equal
probability for each coin.&lt;/p&gt;

&lt;p&gt;Now comes in our mind what we can do with the model in our hands. There
are lot&amp;rsquo;s of stuff to do with it, such as: given a sequence of results,
when the dealer used the biased coin or even generate a random sequence
with a coherent behaviour when compared to the model.&lt;/p&gt;

&lt;p&gt;There is a nice library called &lt;a href=&#34;http://ghmm.org/&#34;&gt;ghmm&lt;/a&gt;
(available for C and Python) which handles HMMs and already gives us
the most famous and important HMM algorithms.
Unfortunately the python wrapper is not pythonic.
Let&amp;rsquo;s model our problem in python to have some fun:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import ghmm 

# setting 0 for Heads and 1 for Tails as our Alphabet
sigma = ghmm.IntegerRange(0, 2)

# transition matrix: rows and columns means origin and destiny states
transitions_probabilities = [
    [0.9, 0.1], # 0: fair state
     [0.1, 0.9], # 1: biased state
]  

# emission matrix: rows and columns means states and symbols respectively 
emissions_probabilities = [
    [0.5, 0.5], # 0: fair state emissions probabilities
    [0.75, 0.25], # 1: biased state emissions probabilities
]

# probability of initial states
pi = [0.5, 0.5]

# equal probabilities for 0 and 1
hmm = ghmm.HMMFromMatrices(
    sigma,
    # you can model HMMs with others emission probability distributions
    ghmm.DiscreteDistribution(sigma),
    transitions_probabilities,
    emissions_probabilities,
    pi )


&amp;gt;&amp;gt;&amp;gt; print(hmm)`\
  DiscreteEmissionHMM(N=2, M=2)
      state 0 (initial=0.50)
          Emissions: 0.50, 0.50
          Transitions: -&amp;gt;0 (0.90), -&amp;gt;1 (0.10)
      state 1 (initial=0.50)
          Emissions: 0.75, 0.25
          Transitions: -&amp;gt;0 (0.10), -&amp;gt;1 (0.90)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have our HMM object on the hand we can play with it. Suppose
you have the given sequence of coin tosses and you would like to
distinguish which coin was being used at a given state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tosses = [1, 1, 1, 0, 1, 0, 0, 1,
          1, 0, 0, 1, 0, 1, 0, 0,
          0, 0, 1, 1, 0, 1, 0, 1,
          0, 0, 0, 1, 0, 0, 0, 0,
          0, 1, 0, 0, 1, 0, 0, 0,
          1, 0, 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;http://en.wikipedia.org/wiki/Viterbi_algorithm&#34;&gt;viterbi algorithm&lt;/a&gt;
can be used to trace the most probable states at each coin toss
according to the HMM distribution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# not as pythonic is could be :-/
sequence = ghmm.EmissionSequence(sigma, tosses)
viterbi_path, _ = hmm.viterbi(sequence)
&amp;gt;&amp;gt;&amp;gt; print(viterbi_path)
  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, , 1, 1, 1, 1, 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nice! But sometimes is interesting to have the probability of each state
on the point instead of only the most probable one. To have that, you
must use the &lt;a href=&#34;http://en.wikipedia.org/wiki/Posterior_mode&#34;&gt;posterior&lt;/a&gt; or
&lt;a href=&#34;http://en.wikipedia.org/wiki/Forward_algorithm&#34;&gt;forward&lt;/a&gt; algorithms to
have more detailed information.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;states_probabilities = hmm.posterior(sequence)
&amp;gt;&amp;gt;&amp;gt; print(states_probabilities)
 [[0.8407944139086141, 0.1592055860913865], [0.860787703168127, 0.13921229683187356], ... ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The posterior method result, returns the list of probabilities at each
state, for example, in the first index we have &lt;code&gt;[0.8407944139086141, 0.1592055860913865]&lt;/code&gt;.
That means that we have ~0.84 probability of chance that the dealer is
using the fair coin and ~0.16 for the biased coin.
We also can plot a graph to show the behaviour of the curve of
the probability of the dealer being using the fair coin
(I used matplotlib for the graphs).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dakerfp.github.io/img/viterbi.png&#34; alt=&#34;Probability of being a fair coin over time&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is only a superficial example of what can HMMs do. It&amp;rsquo;s worthy give
a look at it if you want do some sequence or time series analysis in any
domain. I hope this post presented and cleared what are HMM and how they
can be used to analyse data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>