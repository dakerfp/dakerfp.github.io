<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="theme" content="hugo-academic">
    <meta name="generator" content="Hugo 0.16" />
    <meta name="author" content="Daker Fernandes Pinheiro">
    <meta name="description" content="Software Engineer &amp; MS Candidate for Computer Graphics">

    <link rel="stylesheet" href="http://dakerfp.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="http://dakerfp.github.io/css/bootstrap.min.css">
    <link rel="stylesheet" href="http://dakerfp.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://dakerfp.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
    <link rel="stylesheet" href="http://dakerfp.github.io/css/hugo-academic.css">
    


    <link rel="shortcut icon" href="http://dakerfp.github.io/img/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="http://dakerfp.github.io/post/hidden-markov-models/">

    <title>Hidden Markov Models | Daker Pinheiro</title>

</head>
<body id="top">


<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
    <div class="container">

        
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://dakerfp.github.io/">Daker Pinheiro</a>
        </div>

        
        <div class="collapse navbar-collapse" id="#navbar-collapse-1">

            
            <ul class="nav navbar-nav navbar-right">
                
                <li class="nav-item"><a href="http://dakerfp.github.io/#top">Home</a></li>
                
                <li class="nav-item"><a href="http://dakerfp.github.io/#publications">Publications</a></li>
                
                <li class="nav-item"><a href="http://dakerfp.github.io/#posts">Posts</a></li>
                
                <li class="nav-item"><a href="http://dakerfp.github.io/#projects">Projects</a></li>
                
                <li class="nav-item"><a href="http://dakerfp.github.io/#experience">Experience</a></li>
                
                <li class="nav-item"><a href="http://dakerfp.github.io/#contact">Contact</a></li>
                
            </ul>

        </div>
    </div>
</nav>

<div class="container">

    <article class="article" itemscope itemtype="http://schema.org/Article">

        

        <h1 itemprop="name">Hidden Markov Models</h1>

        

<div class="article-metadata">

    <span class="article-date">
        <time datetime="2011-05-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">Mon, May 30, 2011</time>
    </span>

    
    
    
    <span class="article-tags">
        <i class="fa fa-tags"></i>
        
        <a class="article-tag-link" href="http://dakerfp.github.io/tags/ai">ai</a>, 
        
        <a class="article-tag-link" href="http://dakerfp.github.io/tags/ghmm">ghmm</a>, 
        
        <a class="article-tag-link" href="http://dakerfp.github.io/tags/hmm">hmm</a>, 
        
        <a class="article-tag-link" href="http://dakerfp.github.io/tags/markov">markov</a>, 
        
        <a class="article-tag-link" href="http://dakerfp.github.io/tags/python">python</a>
        
    </span>
    
    

    
        
<div class="share-box">
    <ul class="share">
        <li>
            <a class="facebook" href="https://www.facebook.com/sharer.php?u=http%3a%2f%2fdakerfp.github.io%2fpost%2fhidden-markov-models%2f" target="_blank">
                <i class="fa fa-facebook"></i>
            </a>
        </li>
        <li>
            <a class="twitter" href="https://twitter.com/intent/tweet?text=Hidden%20Markov%20Models&amp;url=http%3a%2f%2fdakerfp.github.io%2fpost%2fhidden-markov-models%2f" target="_blank">
                <i class="fa fa-twitter"></i>
            </a>
        </li>
        <li>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fdakerfp.github.io%2fpost%2fhidden-markov-models%2f&amp;title=Hidden%20Markov%20Models" target="_blank">
                <i class="fa fa-linkedin"></i>
            </a>
        </li>
        <li>
            <a class="weibo" href="http://service.weibo.com/share/share.php?url=http%3a%2f%2fdakerfp.github.io%2fpost%2fhidden-markov-models%2f&amp;title=Hidden%20Markov%20Models" target="_blank">
                <i class="fa fa-weibo"></i>
            </a>
        </li>
        <li>
            <a class="email" href="mailto:?subject=Hidden%20Markov%20Models&amp;body=http%3a%2f%2fdakerfp.github.io%2fpost%2fhidden-markov-models%2f">
                <i class="fa fa-envelope"></i>
            </a>
        </li>
    </ul>
</div>


    

</div>


        <div class="article-style" itemprop="articleBody">
            <p>Nowadays, many applications use
<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Models</a>
(HMMs) to solve crucial issues such as bioinformatics, speech recognition,
musical analysis, digital signal processing, data mining, financial
applications, time series analysis and many others. HMMs are
probabilistic models which are very useful to model sequence behaviours
or discrete time series events. Formally it models
<a href="http://en.wikipedia.org/wiki/Markov_process">Markov processes</a>
with hidden states, like an extension for
<a href="http://en.wikipedia.org/wiki/Markov_chain">Markov Chains</a>.
For computer scientists, is a state machine with probabilistic transitions
where each state can emit a value with a given probability.</p>

<p>For better understanding HMMs, I will illustrate how it works with &ldquo;The
Fair Bet Casino&rdquo; problem. Imagine you are in a casino where you can bet
on coins tosses, tossed by a dealer. A coin toss can have two outcomes:
head (H) or tail (T). Now suppose that the coin dealer has two coins, a
fair (F) which outputs both H and T with <code>1/2</code> probabilities and a biased
coin (B) which outputs H with probability <code>3/4</code> and T with <code>1/4</code>. Using
probability language we say:</p>

<ul>
<li><code>P(H~i+1~|F~i~) = 1/2</code></li>
<li><code>P(T~i+1~|F~i~) = 1/2</code></li>
<li><code>P(H~i+1~|B~i~) = 3/4</code></li>
<li><code>P(T~i+1~|B~i~) = 1/4</code></li>
</ul>

<p>Now imagine that the dealer changes the coin in a way you can&rsquo;t see, but
you know that he does it with a <code>1/10</code> probability. So thinking the coin
tosses as a sequence of events we can say:</p>

<ul>
<li><code>P(F~i+1~|F~i~) = 9/10</code></li>
<li><code>P(B~i+1~|F~i~) = 1/10</code></li>
<li><code>P(B~i+1~|B~i~) = 9/10</code></li>
<li><code>P(F~i+1~|B~i~) = 1/10</code></li>
</ul>

<p>We can model it using a graph to illustrate the process:</p>

<p><img src="http://dakerfp.github.io/img/fairbet.png" alt="HMM for &quot;The Fair Bet Casino&quot; problem" /></p>

<p>That&rsquo;s a HMM! It isn&rsquo;t any rocket science. Is just important to add a
few remarks. We call the set of all possible emissions of the Markov
process as the alphabet Σ ({H, T} in our problem). For many of
computational method involving HMMs you will also need a initial state
distribution π. For our problem we may assume that the we have equal
probability for each coin.</p>

<p>Now comes in our mind what we can do with the model in our hands. There
are lot&rsquo;s of stuff to do with it, such as: given a sequence of results,
when the dealer used the biased coin or even generate a random sequence
with a coherent behaviour when compared to the model.</p>

<p>There is a nice library called <a href="http://ghmm.org/">ghmm</a>
(available for C and Python) which handles HMMs and already gives us
the most famous and important HMM algorithms.
Unfortunately the python wrapper is not pythonic.
Let&rsquo;s model our problem in python to have some fun:</p>

<pre><code>import ghmm 

# setting 0 for Heads and 1 for Tails as our Alphabet
sigma = ghmm.IntegerRange(0, 2)

# transition matrix: rows and columns means origin and destiny states
transitions_probabilities = [
    [0.9, 0.1], # 0: fair state
     [0.1, 0.9], # 1: biased state
]  

# emission matrix: rows and columns means states and symbols respectively 
emissions_probabilities = [
    [0.5, 0.5], # 0: fair state emissions probabilities
    [0.75, 0.25], # 1: biased state emissions probabilities
]

# probability of initial states
pi = [0.5, 0.5]

# equal probabilities for 0 and 1
hmm = ghmm.HMMFromMatrices(
    sigma,
    # you can model HMMs with others emission probability distributions
    ghmm.DiscreteDistribution(sigma),
    transitions_probabilities,
    emissions_probabilities,
    pi )


&gt;&gt;&gt; print(hmm)`\
  DiscreteEmissionHMM(N=2, M=2)
      state 0 (initial=0.50)
          Emissions: 0.50, 0.50
          Transitions: -&gt;0 (0.90), -&gt;1 (0.10)
      state 1 (initial=0.50)
          Emissions: 0.75, 0.25
          Transitions: -&gt;0 (0.10), -&gt;1 (0.90)
</code></pre>

<p>Now that we have our HMM object on the hand we can play with it. Suppose
you have the given sequence of coin tosses and you would like to
distinguish which coin was being used at a given state:</p>

<pre><code>tosses = [1, 1, 1, 0, 1, 0, 0, 1,
          1, 0, 0, 1, 0, 1, 0, 0,
          0, 0, 1, 1, 0, 1, 0, 1,
          0, 0, 0, 1, 0, 0, 0, 0,
          0, 1, 0, 0, 1, 0, 0, 0,
          1, 0, 1]
</code></pre>

<p>The <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm">viterbi algorithm</a>
can be used to trace the most probable states at each coin toss
according to the HMM distribution:</p>

<pre><code># not as pythonic is could be :-/
sequence = ghmm.EmissionSequence(sigma, tosses)
viterbi_path, _ = hmm.viterbi(sequence)
&gt;&gt;&gt; print(viterbi_path)
  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, , 1, 1, 1, 1, 1]
</code></pre>

<p>Nice! But sometimes is interesting to have the probability of each state
on the point instead of only the most probable one. To have that, you
must use the <a href="http://en.wikipedia.org/wiki/Posterior_mode">posterior</a> or
<a href="http://en.wikipedia.org/wiki/Forward_algorithm">forward</a> algorithms to
have more detailed information.</p>

<pre><code>states_probabilities = hmm.posterior(sequence)
&gt;&gt;&gt; print(states_probabilities)
 [[0.8407944139086141, 0.1592055860913865], [0.860787703168127, 0.13921229683187356], ... ]
</code></pre>

<p>The posterior method result, returns the list of probabilities at each
state, for example, in the first index we have <code>[0.8407944139086141, 0.1592055860913865]</code>.
That means that we have ~0.84 probability of chance that the dealer is
using the fair coin and ~0.16 for the biased coin.
We also can plot a graph to show the behaviour of the curve of
the probability of the dealer being using the fair coin
(I used matplotlib for the graphs).</p>

<p><img src="http://dakerfp.github.io/img/viterbi.png" alt="Probability of being a fair coin over time" /></p>

<p>This is only a superficial example of what can HMMs do. It&rsquo;s worthy give
a look at it if you want do some sequence or time series analysis in any
domain. I hope this post presented and cleared what are HMM and how they
can be used to analyse data.</p>

        </div>

    </article>

    <nav>
    <ul class="pager">
        
        <li class="previous"><a href="http://dakerfp.github.io/post/cpg-islands-model-evaluation-1/"><span aria-hidden="true">&larr;</span> CpG Islands (1) - Problem Motivation &amp; Definitions</a></li>
        

        
        <li class="next"><a href="http://dakerfp.github.io/post/cpg-islands-model-evaluation-2/">CpG Islands (2) - Building a Hidden Markov Model <span aria-hidden="true">&rarr;</span></a></li>
        
    </ul>
</nav>

    
<section id="comments">
    <div id="disqus_thread">
        <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'dakerfp';
    var disqus_identifier = 'http:\/\/dakerfp.github.io\/post\/hidden-markov-models\/';
    var disqus_title = 'Hidden Markov Models';
    var disqus_url = 'http:\/\/dakerfp.github.io\/post\/hidden-markov-models\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
</section>



</div>
<footer class="site-footer">
    <div class="container">
        <p class="powered-by">

            &copy; 2016 Daker Fernandes Pinheiro &middot; 

            Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

            <span class="pull-right"><a href="#" id="back_to_top"><span class="button_icon"><i class="fa fa-chevron-up fa-2x" aria-hidden="true"></i></span></a></span>

        </p>
    </div>
</footer>

        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
        <script src="http://dakerfp.github.io/js/jquery-1.12.3.min.js"></script>
        <script src="http://dakerfp.github.io/js/bootstrap.min.js"></script>
        <script src="http://dakerfp.github.io/js/hugo-academic.js"></script>
        

        
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-80351639-1', 'auto');
            ga('send', 'pageview');

             
            var links = document.querySelectorAll('a');
            Array.prototype.map.call(links, function(item) {
                if (item.host != document.location.host) {
                    item.addEventListener('click', function() {
                        var action = item.getAttribute('data-action') || 'follow';
                        ga('send', 'event', 'outbound', action, item.href);
                    });
                }
            });
        </script>
        

        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        
        

    </body>
</html>

