<!DOCTYPE html>
<html lang='en'>

<head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='Nowadays, many applications use Hidden Markov Models (HMMs) to solve crucial issues such as bioinformatics, speech recognition, musical analysis, digital signal processing, data mining, financial applications, time series analysis and many others. HMMs are probabilistic models which are very useful to model sequence behaviours or discrete time series events. Formally it models Markov processes with hidden states, like an extension for Markov Chains. For computer scientists, is a state machine with probabilistic transitions where each state can emit a value with a given probability.'>

<meta property='og:title' content='Hidden Markov Models • Daker'>
<meta property='og:description' content='Nowadays, many applications use Hidden Markov Models (HMMs) to solve crucial issues such as bioinformatics, speech recognition, musical analysis, digital signal processing, data mining, financial applications, time series analysis and many others. HMMs are probabilistic models which are very useful to model sequence behaviours or discrete time series events. Formally it models Markov processes with hidden states, like an extension for Markov Chains. For computer scientists, is a state machine with probabilistic transitions where each state can emit a value with a given probability.'>
<meta property='og:url' content='https://dakerfp.com/post/hidden-markov-models/'>
<meta property='og:site_name' content='Daker Pinheiro'>
<meta property='og:type' content='article'><meta property='article:author' content='https://facebook.com/dakerfp'><meta property='article:publisher' content='https://facebook.com/dakerfp'><meta property='article:section' content='Post'><meta property='article:tag' content='ai'><meta property='article:tag' content='ghmm'><meta property='article:tag' content='hmm'><meta property='article:tag' content='markov'><meta property='article:tag' content='python'><meta property='article:published_time' content='2011-05-30T00:00:00Z'/><meta property='article:modified_time' content='2011-05-30T00:00:00Z'/>

<meta name="generator" content="Hugo 0.27-DEV" />

  <title>Hidden Markov Models • Daker</title>
  <link rel='canonical' href='https://dakerfp.com/post/hidden-markov-models/'>
  
  <link rel='icon' href='/favicon.ico'>
<link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Ubuntu:400,400i,700&subset=latin'>
<link rel='stylesheet' href='/css/main.d02777fd.css'>

  <link rel='stylesheet' href='/css/custom.css'>


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-80351639-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


</head>


<body class='page'>
  <div class='site'>
    <header id='header' class='header-container'>
      <div class='site-header'>
        <nav id='navmenu' aria-label='Main Menu'>
  <ul class='main-menu'>
    
    <li>
      <a href='/' 
        
      >Home</a>
    </li>
    
    <li>
      <a href='/post/' 
        
      >Blog</a>
    </li>
    
  </ul>
</nav>

        <div class='site-info'>
          
          <p class='site-title title'>Daker Pinheiro</p>
          
          <p class='site-description'>My home page &amp; blog</p>
        </div>
      </div>
    </header>


<main class='main'>
  <article lang='en' class='entry'>
    <header class='entry-header'>
  <div class='entry-info'>
    <h1 class='entry-title title'>Hidden Markov Models</h1>
    
  </div>
  
<div class='meta'>
  <span class='posted-on'>
    <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"/>
  <line x1="16" y1="2" x2="16" y2="6"/>
  <line x1="8" y1="2" x2="8" y2="6"/>
  <line x1="3" y1="10" x2="21" y2="10"/>
  
</svg>

    <span class='screen-reader'>Posted on </span>
    <time class='date' datetime='2011-05-30T00:00:00Z'>2011, May 30</time>
  </span>
  
  <span class='byline'>
    <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M21,21V20c0-2.76-4-5-9-5s-9,2.24-9,5v1"/>
  <path d="M16,6.37A4,4,0,1,1,12.63,3,4,4,0,0,1,16,6.37Z"/>
  
</svg>

    <span class='screen-reader'> by </span>
    Daker
  </span>
  
  
  <span class='reading-time'>
    <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <circle cx="12" cy="12" r="10"/>
  <polyline points="12 6 12 12 15 15"/> 
  
</svg>

    5 mins read
  </span>
  
</div>


</header>

    <div class='entry-content'>
  <p>Nowadays, many applications use
<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Models</a>
(HMMs) to solve crucial issues such as bioinformatics, speech recognition,
musical analysis, digital signal processing, data mining, financial
applications, time series analysis and many others. HMMs are
probabilistic models which are very useful to model sequence behaviours
or discrete time series events. Formally it models
<a href="http://en.wikipedia.org/wiki/Markov_process">Markov processes</a>
with hidden states, like an extension for
<a href="http://en.wikipedia.org/wiki/Markov_chain">Markov Chains</a>.
For computer scientists, is a state machine with probabilistic transitions
where each state can emit a value with a given probability.</p>

<p>For better understanding HMMs, I will illustrate how it works with &ldquo;The
Fair Bet Casino&rdquo; problem. Imagine you are in a casino where you can bet
on coins tosses, tossed by a dealer. A coin toss can have two outcomes:
head (H) or tail (T). Now suppose that the coin dealer has two coins, a
fair (F) which outputs both H and T with <code>1/2</code> probabilities and a biased
coin (B) which outputs H with probability <code>3/4</code> and T with <code>1/4</code>. Using
probability language we say:</p>

<ul>
<li><code>P(H~i+1~|F~i~) = 1/2</code></li>
<li><code>P(T~i+1~|F~i~) = 1/2</code></li>
<li><code>P(H~i+1~|B~i~) = 3/4</code></li>
<li><code>P(T~i+1~|B~i~) = 1/4</code></li>
</ul>

<p>Now imagine that the dealer changes the coin in a way you can&rsquo;t see, but
you know that he does it with a <code>1/10</code> probability. So thinking the coin
tosses as a sequence of events we can say:</p>

<ul>
<li><code>P(F~i+1~|F~i~) = 9/10</code></li>
<li><code>P(B~i+1~|F~i~) = 1/10</code></li>
<li><code>P(B~i+1~|B~i~) = 9/10</code></li>
<li><code>P(F~i+1~|B~i~) = 1/10</code></li>
</ul>

<p>We can model it using a graph to illustrate the process:</p>

<p><img src="/img/fairbet.png" alt="HMM for &quot;The Fair Bet Casino&quot; problem" /></p>

<p>That&rsquo;s a HMM! It isn&rsquo;t any rocket science. Is just important to add a
few remarks. We call the set of all possible emissions of the Markov
process as the alphabet Σ ({H, T} in our problem). For many of
computational method involving HMMs you will also need a initial state
distribution π. For our problem we may assume that the we have equal
probability for each coin.</p>

<p>Now comes in our mind what we can do with the model in our hands. There
are lot&rsquo;s of stuff to do with it, such as: given a sequence of results,
when the dealer used the biased coin or even generate a random sequence
with a coherent behaviour when compared to the model.</p>

<p>There is a nice library called <a href="http://ghmm.org/">ghmm</a>
(available for C and Python) which handles HMMs and already gives us
the most famous and important HMM algorithms.
Unfortunately the python wrapper is not pythonic.
Let&rsquo;s model our problem in python to have some fun:</p>

<pre><code>import ghmm 

# setting 0 for Heads and 1 for Tails as our Alphabet
sigma = ghmm.IntegerRange(0, 2)

# transition matrix: rows and columns means origin and destiny states
transitions_probabilities = [
    [0.9, 0.1], # 0: fair state
     [0.1, 0.9], # 1: biased state
]  

# emission matrix: rows and columns means states and symbols respectively 
emissions_probabilities = [
    [0.5, 0.5], # 0: fair state emissions probabilities
    [0.75, 0.25], # 1: biased state emissions probabilities
]

# probability of initial states
pi = [0.5, 0.5]

# equal probabilities for 0 and 1
hmm = ghmm.HMMFromMatrices(
    sigma,
    # you can model HMMs with others emission probability distributions
    ghmm.DiscreteDistribution(sigma),
    transitions_probabilities,
    emissions_probabilities,
    pi )


&gt;&gt;&gt; print(hmm)`\
  DiscreteEmissionHMM(N=2, M=2)
      state 0 (initial=0.50)
          Emissions: 0.50, 0.50
          Transitions: -&gt;0 (0.90), -&gt;1 (0.10)
      state 1 (initial=0.50)
          Emissions: 0.75, 0.25
          Transitions: -&gt;0 (0.10), -&gt;1 (0.90)
</code></pre>

<p>Now that we have our HMM object on the hand we can play with it. Suppose
you have the given sequence of coin tosses and you would like to
distinguish which coin was being used at a given state:</p>

<pre><code>tosses = [1, 1, 1, 0, 1, 0, 0, 1,
          1, 0, 0, 1, 0, 1, 0, 0,
          0, 0, 1, 1, 0, 1, 0, 1,
          0, 0, 0, 1, 0, 0, 0, 0,
          0, 1, 0, 0, 1, 0, 0, 0,
          1, 0, 1]
</code></pre>

<p>The <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm">viterbi algorithm</a>
can be used to trace the most probable states at each coin toss
according to the HMM distribution:</p>

<pre><code># not as pythonic is could be :-/
sequence = ghmm.EmissionSequence(sigma, tosses)
viterbi_path, _ = hmm.viterbi(sequence)
&gt;&gt;&gt; print(viterbi_path)
  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, , 1, 1, 1, 1, 1]
</code></pre>

<p>Nice! But sometimes is interesting to have the probability of each state
on the point instead of only the most probable one. To have that, you
must use the <a href="http://en.wikipedia.org/wiki/Posterior_mode">posterior</a> or
<a href="http://en.wikipedia.org/wiki/Forward_algorithm">forward</a> algorithms to
have more detailed information.</p>

<pre><code>states_probabilities = hmm.posterior(sequence)
&gt;&gt;&gt; print(states_probabilities)
 [[0.8407944139086141, 0.1592055860913865], [0.860787703168127, 0.13921229683187356], ... ]
</code></pre>

<p>The posterior method result, returns the list of probabilities at each
state, for example, in the first index we have <code>[0.8407944139086141, 0.1592055860913865]</code>.
That means that we have ~0.84 probability of chance that the dealer is
using the fair coin and ~0.16 for the biased coin.
We also can plot a graph to show the behaviour of the curve of
the probability of the dealer being using the fair coin
(I used matplotlib for the graphs).</p>

<p><img src="/img/viterbi.png" alt="Probability of being a fair coin over time" /></p>

<p>This is only a superficial example of what can HMMs do. It&rsquo;s worthy give
a look at it if you want do some sequence or time series analysis in any
domain. I hope this post presented and cleared what are HMM and how they
can be used to analyse data.</p>

</div>

    
<footer class='entry-footer'>
  
    
  
    
      
      

<div class='tags'>
  <span class='tag-icon'>
    <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M20.59,13.41l-7.17,7.17a2,2,0,0,1-2.83,0L2,12V2H12l8.59,8.59A2,2,0,0,1,20.59,13.41Z"/>
  <line x1="7" y1="7" x2="7" y2="7"/>
  
</svg>

  </span>
  <span class='screen-reader'>Tags: </span><a class='tag' href='/tags/ai'>ai</a>, <a class='tag' href='/tags/ghmm'>ghmm</a>, <a class='tag' href='/tags/hmm'>hmm</a>, <a class='tag' href='/tags/markov'>markov</a>, <a class='tag' href='/tags/python'>python</a></div>

    
  
</footer>


  </article>

  
    
<nav class='entry-nav'>
  <div class='entry-nav-links'><div class='prev-entry'>
      <a href='https://dakerfp.com/post/cpg-islands-model-evaluation-1/'>
        <span aria-hidden='true'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="20" y1="12" x2="4" y2="12"/>
  <polyline points="10 18 4 12 10 6"/>
  
</svg>
 Previous</span>
        <span class='screen-reader'>Previous post: </span>CpG Islands (1) - Problem Motivation &amp; Definitions</a>
    </div><div class='next-entry'>
      <a href='https://dakerfp.com/post/cpg-islands-model-evaluation-2/'>
        <span class='screen-reader'>Next post: </span>CpG Islands (2) - Building a Hidden Markov Model<span aria-hidden='true'>Next <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="4" y1="12" x2="20" y2="12"/>
  <polyline points="14 6 20 12 14 18"/>
  
</svg>
</span>
      </a>
    </div></div>
</nav>


  

  
    <div class='comments-container'>
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "dakerfp" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

  
</main>

    <footer id='footer' class='footer-container'>
      <div class='footer'>
        <div class='social'>
  <nav aria-label='Social Menu'>
    <ul class='social-menu'><li>
        <a href='https://facebook.com/dakerfp' target='_blank' rel='noopener'>
          <span class='screen-reader'>Open Facebook account in new tab</span>
          <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"/>
  
</svg>

        </a>
      </li><li>
        <a href='https://github.com/http://github.com/dakerfp' target='_blank' rel='noopener'>
          <span class='screen-reader'>Open Github account in new tab</span>
          <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
  
</svg>

        </a>
      </li><li>
        <a href='https://twitter.com/dakerfp' target='_blank' rel='noopener'>
          <span class='screen-reader'>Open Twitter account in new tab</span>
          <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
  
</svg>

        </a>
      </li></ul>
  </nav>
</div>

        <div class='copyright'>
          <p>Ⓒ 2017 Daker Fernandes Pinheiro</p>

        </div>
      </div>
    </footer>

  </div>

  <script src='/js/main.af838dd5.js'></script>
  
    <script src='/js/custom.js'></script>
  

</body>

</html>

